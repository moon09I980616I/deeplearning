### ****경사 하강법 구현하기****

```jsx
optimizer = optim.SDG([W, b], lr=0.01)
```

`optimizer.zero_grad()`를 실행함으로써 미분을 통해 얻은 기울기를 0으로 초기화한다. 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있다.

그 다음 `cost.backward()` 함수를 호출하면 가중치 $W$와 편향 $b$에 대한 기울기가 계산된다.

그 다음 경사 하강법 최적화 함수 optimizer의 `.step()` 함수를 호출하여 인수로 들어갔던 $W$와 $b$에서 리턴되는 변수들의 기울기에 학습률(learning rate) 0.01을 곱하여 빼줌으로서 업데이트한다.

```jsx
# gradient를 0으로 초기화
optimizer.zero_grad()
# 비용 함수를 미분하여 gradient 계산
cost.backward()
# W와 b를 업데이트
optimizer.step()
```

### optimizer.zero_grad()가 필요한 이유

파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있다.

```jsx
import torch
w = torch.tensor(2.0, requires_grad=True)

nb_epochs = 20
for epoch in range(nb_epochs + 1):

  z = 2*w

  z.backward()
  print('수식을 w로 미분한 값 : {}'.format(w.grad))
```

```jsx
수식을 w로 미분한 값 : 2.0
수식을 w로 미분한 값 : 4.0
수식을 w로 미분한 값 : 6.0
수식을 w로 미분한 값 : 8.0
수식을 w로 미분한 값 : 10.0
수식을 w로 미분한 값 : 12.0
수식을 w로 미분한 값 : 14.0
수식을 w로 미분한 값 : 16.0
수식을 w로 미분한 값 : 18.0
수식을 w로 미분한 값 : 20.0
수식을 w로 미분한 값 : 22.0
수식을 w로 미분한 값 : 24.0
수식을 w로 미분한 값 : 26.0
수식을 w로 미분한 값 : 28.0
수식을 w로 미분한 값 : 30.0
수식을 w로 미분한 값 : 32.0
수식을 w로 미분한 값 : 34.0
수식을 w로 미분한 값 : 36.0
수식을 w로 미분한 값 : 38.0
수식을 w로 미분한 값 : 40.0
수식을 w로 미분한 값 : 42.0
```

계속해서 미분값인 2가 누적된다.

그렇기 때문에 `optimizer.zero_grad()`를 통해 미분값을 계속 0으로 초기화시켜줘야 합니다.

### ****torch.manual_seed()를 하는 이유****

torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해준다

```jsx
import torch
torch.manual_seed(3)
print('랜덤 시드가 3일 때')
for i in range(1,3):
  print(torch.rand(1))

torch.manual_seed(5)
print('랜덤 시드가 5일 때')
for i in range(1,3):
  print(torch.rand(1))
```

```jsx
랜덤 시드가 3일 때
tensor([0.0043])
tensor([0.1056])

랜덤 시드가 5일 때
tensor([0.8303])
tensor([0.1261])
```

랜덤 시드 값을 다시 3으로 돌렸을 때 프로그램을 다시 처음부터 실행한 것처럼 난수 발생 순서가 초기화된다.

```jsx
torch.manual_seed(3)
print('랜덤 시드가 다시 3일 때')
for i in range(1,3):
  print(torch.rand(1))
```

```jsx
랜덤 시드가 다시 3일 때
tensor([0.0043])
tensor([0.1056])
```
