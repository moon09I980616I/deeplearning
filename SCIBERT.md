# SCIBERT
---

논문 링크 : https://arxiv.org/abs/1903.10676

코드 및 모델 : https://github.com/allenai/scibert/

---
사전 훈련된 언어 모델 BERT를 기반으로 SCIBERT 만듬

다양한 과학 영역의 데이터 세트 사용

BERT와 동일한 아키텍처에 과학 텍스트로 학습

- BERT 어휘 방식 :  가장 자주 사용되는 단어 or subword units 포함
- 어휘 크기 : 30K (대소문자 구분 x, BASE VOCAB과 크기 일치)
- BASE VOCAB과 토큰 중첩은 42% -> 과학 도메인과 일반 도메인 차이 큼
- corpus 구성 : **생물 의학 영역 논문(82%), 컴퓨터 과학 영역(18%**) //논문량 1.14M
- 논문 길이 : 평균 154문장(2,769개 토큰) -> 3.3B 토큰과 유사한 3.17B 토큰
- 문장 분할 방식 : ScispaCy2 사용

BERT를 기반으로 하지만 대규모 과학 말뭉치에 대해 학습된 대규모 과학 텍스트 말뭉치에 대해 학습된 언어 모델입니다

**과학텍스트에 최적화되어있는 SCIBERT**
